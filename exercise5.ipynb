{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hinton's Capsule Networks\n",
    "\n",
    "<div style = \"text-align:justify\"> We have given a brief intro to Capsules already. In this notebook we will describe the dynamic routing algorithm between capsules and a capsule net architecture called CapsNet for recognizing and reconstructing handwritten digits from MNIST dataset. We will also implement this architecture. </div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, let's load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 78,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "The autoreload extension is already loaded. To reload it, use:\n",
      "  %reload_ext autoreload\n"
     ]
    }
   ],
   "source": [
    "#run this cell\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader, sampler\n",
    "from torch.optim import lr_scheduler\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time\n",
    "import torchvision.utils\n",
    "from capsnet_utils import *\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\"> Think of capsule as a group of neurons. In traditional CNN's, the activity of a neuron is a scalar. But the activity or the ouput of a capsule is a vector. So, if a capsule is made up of 8 neurons, its activity is a 8 dimensional vector. Length of the vector will be the probability of presence of an entity/feature. It's orientation encodes other instantiation parameters associated with the entity like pose, illumination, deformation etc. Since the length has to be probability it has to be squashed between 0 and 1. We use the following non-linear function to squash the length.</div>\n",
    "<br>\n",
    "<center>$v_j=\\frac{||s_j||^2}{1+||s_j||^2}\\frac{s_j}{||s_j||}\\tag{1}$</center> \n",
    "\n",
    "<div style = \"text-align:justify\">Let's write this squash function. One point to keep in mind is that we will write a vectorized code that squashes lengths of all capsules in a particular layer for the entire mini-batch. So $s_j$ will of dimension *batch_size x #capsules in the layer x capsule_dim x 1.*</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def my_squash(s, dim = 2):\n",
    "    \n",
    "    s_norm = torch.norm(s, p = 2, dim = dim, keepdim = True)\n",
    "    s_norm_sqr = s_norm ** 2     \n",
    "    scalar = s_norm_sqr / (1. + s_norm_sqr)\n",
    "    v = scalar * (s / s_norm)\n",
    "    return v"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\"> Like how neurons in a lower layer connect to neurons in higher layer, lower layer capsules connect to capsules in higher layer. But how? First, each capsule predicts the activity of every capsule in the higher layer. This prediction is obtained by multiplying the activity vector of the lower layer capsule with a weight matrix that ouputs an prediction vector for the higher layer capsule. See Fig 1.</div> \n",
    "\n",
    "<img src=\"images/capsule_vs_ordinary.jpeg\" style=\"width:450px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **Capsule vs Ordinary neuron [[source]](https://medium.com/ai%C2%B3-theory-practice-business/understanding-hintons-capsule-networks-part-ii-how-capsules-work-153b6ade9f66)** </center></caption>\n",
    "<br>\n",
    "<div style = \"text-align:justify\"> Let us say $u_1$, $u_2$ and $u_3$ are activity vectors of 3 capsules in the lower layer. Consider capsule j in the higher layer. $u_1$ may encode position of nose, $u_2$ may encode mouth and $u_3$ may encode right eye. Capsule j in the higher layer may encode a face. Matrix $w_{ij}$ encodes the relationship between $u_1$ and capsule j, for eg that face is centred around nose, face is 10 times bigger than nose and its orientation corresponds to orientation of the nose. Similarly for other capsules. In other words the predictions, say, $\\hat{u}_{j|1}$, $\\hat{u}_{j|2}$ and $\\hat{u}_{j|3}$ represent where the face should be according to the detected positions of nose, mouth and right eye respectively. Mathematically,</div>\n",
    "<center>$\\hat{u}_{j|i} = W_{ij}u_{i}$</center>\n",
    "This was missing in CNN!! \n",
    "<div style = \"text-align:justify\"> Let's code this. Note that again we will be writing a vectorized code. $W$ is tensor of dimension *batch_size x # capsules in lower_layer x # capsules in higher_layer x capsule_dim in higher layer x capsule_dim in lower layer.* Dimension of $u$ is *batch_size x # capsules in lower_layer x capsule_dim of lower layer x 1* \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict_activity(W, u):\n",
    "    \n",
    "    num_capsules_higher = W.size(2)\n",
    "    \n",
    "    # size of W: batch_size x # capsules in lower_layer x # capsules in higher_layer \n",
    "    #                       x capsule_dim in higher layer x capsule_dim in lower layer\n",
    "    # size of u: batch_size x # capsules in lower_layer  \n",
    "    #                       x capsule_dim in lower layer x 1\n",
    "    # if we stack u along dim 2 # capsules in higher_layer times, size of u will become\n",
    "    #           batch_size x # capsules in lower_layer x # capsules in higher_layer\n",
    "    #                      x capsule_dim in lower layer x 1\n",
    "    # so we can directly invoke torch.matmul to do the multiplication of two tensors and get u_hat whose size is\n",
    "    #           batch_size x # capsules in lower_layer x # capsules in higher_layer \n",
    "    #                      x capsule_dim in higher layer x 1\n",
    "    \n",
    "    u = torch.stack([u]*num_capsules_higher, dim = 2)\n",
    "    u_hat = torch.matmul(W, u) \n",
    "    return u_hat"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\">Individual predictions about higher layer capsules by lower layer capsules are available. We need to decide which capsules in higher layer should be coupled with which capsules in lower layer. Suppose capsule i in lower layer encodes position of nose that  is oriented frontal. And, say the three capsules in higher layer encode a frontal face, a profile face and an aeroplane. Then clearly capsule i should be coupled with first capsule in the higher layer. This is the essence of dynamic routing algorithm. It couples capsules between layers that agree with each other. More they agree, higher is the probability of coupling. !!Remember the inverse graphics!! - the visual scene is deconstructed into patterns/features,then matched with agreeable higher level features to recognize the scene. Coupling probabilities are denoted by $c_{ij}$. Note that</div>\n",
    "<center>$\\sum_{j}c_{ij} = 1$</center>\n",
    "<div style = \"text-align:justify\"> Since $c_{ij}$'s are probabilities for each i, we will compute them using softmax. i.e </div>\n",
    "<center>$c_{ij} = \\frac{e^{b_{ij}}}{\\sum_{j}{e^{b_{ij}}}}\\tag{3}$</center> where $b_{ij}$'s are initial priors that are learnt discriminatively at the same time as all other weights.\n",
    "<div style = \"text-align:justify\"> The final prediction $s_j$ for capsule j will be  a weighted linear combination of the individual predictions. That is,</div>\n",
    "<center>$s_j=\\sum_{i}c_{ij}\\hat{u}_{j|i}$</center> <br> \n",
    "<div style = \"text-align:justify\"> $v_j$'s are squashed $s_j$'s.</div>\n",
    "Priors $b_{ij}$ are updated iteratively as  \n",
    "<center>$b_{ij}\\,\\,+=\\, \\hat{u}_{j|i}^T\\,v_j$</center> where the last term which is a dot product quantifies the agreement between capsule i and capsule j. The number of routing iterations will be denoted by r.\n",
    "<br>\n",
    "The dynamic routing algorithm is shown in Fig 2. Let's code the dynamic routing algorithm.\n",
    "<img src=\"images/routing.png\" style=\"width:550px;height:250px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 2** </u><font color='purple'>  : **Dynamic Routing Algorithm [[source]](https://arxiv.org/pdf/1710.09829.pdf)** </center></caption>\n",
    "<br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def dynamic_routing(u_hat, r):\n",
    "    \"\"\"\n",
    "    Ip:\n",
    "        u_hat: batch_size x #caps in layer l x #caps in layer l+1 x cap_dim in layer l+1 x 1\n",
    "        r: number of routing iterations    \n",
    "    Return:\n",
    "        v: activity vectors in layer l+1.\n",
    "           size of v is batch_size x #caps in layer l+1 x cap_dim in layer l+1 x 1\n",
    "    \"\"\"\n",
    "    batch_size = u_hat.size(0)\n",
    "    b = Variable(torch.zeros(1, u_hat.size(1), u_hat.size(2), 1))\n",
    "    if torch.cuda.is_available():\n",
    "        b = b.cuda()\n",
    "    for riter in range(r):\n",
    "        c = softmax(b, dim = 1) #see eqn(3)\n",
    "        c = torch.cat([c] * batch_size, dim = 0).unsqueeze(4) # to take advantage of python \n",
    "                                                                # broadcasting to do step 5 in Fig 2\n",
    "        s = torch.sum(c * u_hat, dim = 1) # step 5\n",
    "        v = my_squash(s, dim = 2) # step 6\n",
    "        v_temp = torch.stack([v] * u_hat.size(1), dim = 1) # required for vectorizing step 7 in Fig 2\n",
    "        b = b + torch.matmul(u_hat.transpose(3, 4), v_temp).squeeze(4).mean(dim = 0, keepdim = True)\n",
    "                                                           # step 7\n",
    "    return v  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\"> Let's come to the CapsNet architecture for handwritten digit recognition. It is shown in Fig 3.</div>\n",
    "<img src=\"images/capsnet1.png\" style=\"width:650px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 3** </u><font color='purple'>  : **CapsNet  architecture [[source]](https://arxiv.org/pdf/1710.09829.pdf)** </center></caption>\n",
    "<br>\n",
    "<div style = \"text-align:justify\"> It takes 28 x 28 gray scale input. A standard ReLU convolution layer follows this with 256 filters each of size 9 x 9 with stride size = (1, 1) and no padding. So the output size is *batch_size x 256 x 20 x 20.* Then comes the two capsule layers - the primary capsule layer and the next level capsule layer called as digits capsule layer.</div>\n",
    "<br>\n",
    "Characterisitcs of primary capsule layer are as follows:\n",
    "- Each capsule is 8D. Each capsule is made up of 8 ReLU conv units with filter size 9 x 9 and stride size =                                                                                     (2, 2). No padding. So each conv unit will reduce 20 x 20 to 6 x 6\n",
    "- There are 32 blocks of capsules. So, the total number of capsules are 32 \\* 6 \\* 6 = 1152\n",
    "\n",
    "Characterisitcs of digits capsule layer are as follows:\n",
    "- Each capsule is 16D\n",
    "- There are 10 capsules corresponding to 10 digits\n",
    "- $W_{ij}$ is a 8 x 16 matrix required for capsule i in primary layer to predict capsule j in digits capsule layer\n",
    "- digits capsule layers activities are based on routing. No convolutional/dense units involved.\n",
    "\n",
    "Note that digits capsule layer is based on routing while primary capsule layer gets input from previous convolutional layer.\n",
    "\n",
    "Let's code *CapsuleLayer* class and then *CapsNet* class which uses the former class."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class CapsuleLayer(nn.Module):\n",
    "    \n",
    "    def __init__(self, use_routing = True):\n",
    "        \n",
    "        super().__init__()\n",
    "        self.use_routing = use_routing\n",
    "        if not self.use_routing: # create primary capsule layer\n",
    "            self.conv_units = nn.ModuleList([nn.Conv2d(256, 32, kernel_size = (9, 9),\n",
    "                                                          stride = (2, 2)) for i in range(8)])\n",
    "        else: # digits capsule; so create W as a learnable set of parameters initialized randomly\n",
    "            self.W = nn.Parameter(torch.randn(1, 1152, 10, 16, 8)) # 1st dim is 1 since right now we dont\n",
    "                                                                   # know batch_size\n",
    "            self.routing_iterations = 3\n",
    "    \n",
    "    def forward(self, x):\n",
    "        \n",
    "        if not self.use_routing:  # forward prop through primary capsule layer          \n",
    "            outputs = [m(x) for i, m in enumerate(self.conv_units)] # a list of 8 outputs\n",
    "            outputs = torch.stack(outputs, dim = 4) # stack all of them along dim 4\n",
    "            #assert capsules.size() == [x.size(0), 32, 6, 6, 8]\n",
    "            s = outputs.view(x.size(0), -1, 8).unsqueeze(dim = 3) \n",
    "                                                # reshape to size batch_size x 1152 x 8 x 1\n",
    "            v = my_squash(s, 2) # squash along dim 2           \n",
    "            #assert capsules.size() == [x.size(0), 1152, 8, 1]\n",
    "            return v\n",
    "        \n",
    "        else: #forward prop through digits capsule by prediciting activity and routing\n",
    "            u_hat = predict_activity(self.W, x)\n",
    "            v = dynamic_routing(u_hat, self.routing_iterations)\n",
    "            return v   \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The CapsNet class...."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#self explanatory - For a particular reason which will be explained later we will call this CapsNet1\n",
    "class CapsNet1(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        \n",
    "        super().__init__()       \n",
    "        self.conv = nn.Conv2d(1, 256, kernel_size = (9, 9), stride = (1, 1))        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.primary_capsule = CapsuleLayer(use_routing = False)\n",
    "        self.digits_capsule = CapsuleLayer(use_routing = True)        \n",
    "               \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)        \n",
    "        x = self.relu(x)\n",
    "        x = self.primary_capsule(x)\n",
    "        x = self.digits_capsule(x)            \n",
    "        return x       \n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\"> Now we need to compute loss. We have two types of losses - margin loss for digit recognition and reconstruction loss for digit reconstruction. Margin Loss is defined as shown in the following equation.\n",
    "<img src=\"images/margin_loss.png\" style=\"width:450px;height:40px;\">\n",
    "<br>\n",
    "where $T_k$ is 1 iff digit of class $k$ is present, $m^+ = 0.9$, $m^- = 0.1$ and $\\lambda = 0.5$. The total margin loss is simply the sum of the losses of all digit capsules.\n",
    "\n",
    "Let's code this loss."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#self explanatory - For a particular reason which will be explained later we will call this margin_loss1\n",
    "def margin_loss1(v, labels, size_average = True):\n",
    "    \n",
    "        \"\"\"\n",
    "        Ip:\n",
    "        v is batch_size x 10 x 16 x 1\n",
    "        labels is batch_size x 10 (one_hot_encoded labels)\n",
    "        \n",
    "        Returns:\n",
    "        L which is the margin loss\n",
    "        \"\"\"\n",
    "        m_plus = 0.9\n",
    "        m_minus = 0.1\n",
    "        lambd = 0.5        \n",
    "        norm_v = torch.norm(v, p = 2, dim = 2).squeeze() # norm_v is batch_size x 10\n",
    "        Lk_first_term = m_plus - norm_v\n",
    "        Lk_first_term[Lk_first_term < 0] = 0\n",
    "        Lk_first_term = Lk_first_term ** 2        \n",
    "        Lk_first_term = Lk_first_term * labels\n",
    "\n",
    "        Lk_second_term = norm_v - m_minus\n",
    "        Lk_second_term[Lk_second_term < 0] = 0\n",
    "        Lk_second_term = Lk_second_term ** 2\n",
    "        Lk_second_term = lambd * (1 - labels) * Lk_second_term \n",
    "\n",
    "        Lk = Lk_first_term + Lk_second_term\n",
    "        L = torch.sum(L, dim = 1)\n",
    "\n",
    "        if size_average: # average over the batch\n",
    "            L = L.mean()\n",
    "        return L\n",
    "        "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text_align:justify\"> For reconstruction loss, we will use MSE loss. The combined loss is defined as margin_loss + 0.0005 \\* reconstruction_loss.</div>\n",
    "\n",
    "<div style = \"text_align:justify\">But before we code reconstruction loss we need to reconstruct the image from the output vectors of the digits capsule layer. Towards this we will use the decoder shown in Fig 4.</div>\n",
    "<img src=\"images/capsnet2.png\" style=\"width:450px;height:150px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 4** </u><font color='purple'>  : **Decoder for reconstruction [[source]](https://arxiv.org/pdf/1710.09829.pdf)** </center></caption>\n",
    "<br>\n",
    "<div style = \"text_align:justify\"> In Fig 4, in the first block, orange color indicates the activity vector with the largest length while the grayish blue indicates activities masked to zero. The activity with largest length is forwarded while other activity vectors are masked to zero. First we will code the reconstruction loss assuming we get the ouput from the decoder. Then we will code the decoder as part of *CapsNet* class and use it whenever the boolean variable *use_reconstruction* is set. We are going to make *margin_loss* and *reconstruction_loss* defined above as part of *CapsNet* class for ease of implementation. </div> "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#self explanatory - For a particular reason which will be explained later we will call this reconstruction_loss1\n",
    "def reconstruction_loss1(x, images, size_average = True):\n",
    "    \"\"\"\n",
    "    Ip:\n",
    "    x is batch_size x 784\n",
    "    images is batch_size x 28 x 28\n",
    "    \n",
    "    Returns:\n",
    "    L which is reconstruction loss\n",
    "    \"\"\"\n",
    "    L = (x - images.view(x.size(0), -1)) ** 2\n",
    "    L = torch.sum(L, dim = 1)\n",
    "    if size_average:\n",
    "        L = L.mean()\n",
    "    return L"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "#self explanatory - now it should be clear why we called above class as CapsNet1\n",
    "     # and the methods as margin_loss1 and reconstruction_loss1\n",
    "    \n",
    "class CapsNet(nn.Module):\n",
    "    \n",
    "    def __init__(self, use_reconstruction):\n",
    "        \n",
    "        super().__init__() \n",
    "        self.use_reconstruction = use_reconstruction\n",
    "        self.conv = nn.Conv2d(1, 256, kernel_size = (9, 9), stride = (1, 1))        \n",
    "        self.relu = nn.ReLU()\n",
    "        self.primary_capsule = CapsuleLayer(use_routing = False)\n",
    "        self.digits_capsule = CapsuleLayer(use_routing = True) \n",
    "        \n",
    "        if self.use_reconstruction:\n",
    "            self.fc1 = nn.Linear(10 * 16, 512)\n",
    "            self.relu1 = nn.ReLU()\n",
    "            self.fc2 = nn.Linear(512, 1024)\n",
    "            self.relu2 = nn.ReLU()\n",
    "            self.fc3 = nn.Linear(1024, 784)\n",
    "            self.sigmoid = nn.Sigmoid()\n",
    "                \n",
    "               \n",
    "    def forward(self, x):\n",
    "        \n",
    "        x = self.conv(x)        \n",
    "        x = self.relu(x)\n",
    "        x = self.primary_capsule(x)\n",
    "        x = self.digits_capsule(x) \n",
    "        return x\n",
    "    \n",
    "    def model_loss(self, x, labels, images, size_average = True):\n",
    "        mloss = self.margin_loss(x, labels, size_average)\n",
    "        loss = mloss\n",
    "        reconstructed = None\n",
    "        if self.use_reconstruction:\n",
    "            x = mask(x)\n",
    "            x = x.view(-1, 10 * 16 * 1)\n",
    "            x = self.fc1(x)\n",
    "            x = self.relu1(x)\n",
    "            x = self.fc2(x)\n",
    "            x = self.relu2(x)\n",
    "            x = self.fc3(x)\n",
    "            reconstructed = self.sigmoid(x)\n",
    "            rloss = self.reconstruction_loss(reconstructed, images, size_average)\n",
    "            loss = mloss + 0.0005 * rloss\n",
    "        return loss, reconstructed \n",
    "    \n",
    "    def margin_loss(self, v, labels, size_average = True):\n",
    "    \n",
    "        \"\"\"\n",
    "        Ip:\n",
    "        v is batch_size x 10 x 16 x 1\n",
    "        labels is batch_size x 10 (one_hot_encoded labels)\n",
    "\n",
    "        Returns:\n",
    "        L which is the margin loss\n",
    "        \"\"\"\n",
    "        m_plus = 0.9\n",
    "        m_minus = 0.1\n",
    "        lambd = 0.5        \n",
    "        norm_v = torch.norm(v, p = 2, dim = 2).squeeze() # norm_v is batch_size x 10\n",
    "        Lk_first_term = m_plus - norm_v\n",
    "        Lk_first_term[Lk_first_term < 0] = 0\n",
    "        Lk_first_term = Lk_first_term ** 2        \n",
    "        Lk_first_term = Lk_first_term * labels\n",
    "\n",
    "        Lk_second_term = norm_v - m_minus\n",
    "        Lk_second_term[Lk_second_term < 0] = 0\n",
    "        Lk_second_term = Lk_second_term ** 2\n",
    "        Lk_second_term = lambd * (1 - labels) * Lk_second_term \n",
    "\n",
    "        Lk = Lk_first_term + Lk_second_term\n",
    "        L = torch.sum(Lk, dim = 1)\n",
    "\n",
    "        if size_average: # average over the batch\n",
    "            L = L.mean()\n",
    "        return L\n",
    "    \n",
    "    def reconstruction_loss(self, x, images, size_average = True):\n",
    "        \"\"\"\n",
    "        Ip:\n",
    "        x is batch_size x 784\n",
    "        images is batch_size x 28 x 28\n",
    "\n",
    "        Returns:\n",
    "        L which is reconstruction loss\n",
    "        \"\"\"\n",
    "        L = (x - images.view(x.size(0), -1)) ** 2\n",
    "        L = torch.sum(L, dim = 1)\n",
    "        if size_average:\n",
    "            L = L.mean()\n",
    "        return L"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All set for training and testing...\n",
    "<br>\n",
    "Let's load data first."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===> Loading training datasets\n",
      "===> Loading testing datasets\n"
     ]
    }
   ],
   "source": [
    "train_loader, test_loader = load_mnist()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, optimizer, save_file, scheduler = None):\n",
    "    \n",
    "    since = time.time()\n",
    "    train_loss_history = [] \n",
    "    num_epochs = model.num_epochs\n",
    "    num_batches = len(data_loader)\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train(True)\n",
    "        if scheduler:\n",
    "            scheduler.step()\n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for inputs, labels in data_loader:\n",
    "            labels = one_hot_encode(labels, 10)\n",
    "            inputs, labels = Variable(inputs), Variable(labels)\n",
    "            #do not uncomment the four lines below; we will be working with CPU\n",
    "            \"\"\"\n",
    "            if torch.cuda.is_available():\n",
    "                inputs, labels = Variable(inputs.cuda()), Variable(labels.cuda())\n",
    "            else:\n",
    "                inputs, labels = Variable(inputs), Variable(labels)\n",
    "            \"\"\"\n",
    "            optimizer.zero_grad()\n",
    "            outputs = model(inputs)\n",
    "            loss, reconstructed = model.model_loss(outputs, labels, inputs)\n",
    "                    \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.data[0]\n",
    "             \n",
    "        epoch_loss = running_loss / num_batches           \n",
    "        train_loss_history.append(epoch_loss)        \n",
    "        print('Train Loss: {:.8f}'.format(epoch_loss))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))\n",
    "    torch.save(model.state_dict(), save_file)\n",
    "    torch.save(train_loss_history, save_file + '_loss_history')\n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# train the model - each epoch takes around 4 to 5 minutes in Titan-X GPU\n",
    "# we have a pre-trianed model for you\n",
    "#you need not train\n",
    "caps_net = CapsNet(use_reconstruction = True)\n",
    "optimizer = torch.optim.Adam(caps_net.parameters(), lr = 0.01)\n",
    "exp_lr_scheduler = lr_scheduler.StepLR(optimizer, step_size = 10, gamma = 0.95)\n",
    "caps_net.num_epochs = 1\n",
    "#if torch.cuda.is_available():\n",
    " #   caps_net.cuda()\n",
    "#train_model(caps_net, training_data_loader, optimizer, 'caps_net_1.pth') "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's test for accuracy and also see the reconstructed images."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 83,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "model = CapsNet(use_reconstruction = True)\n",
    "#test_model(model, 'caps_net_100.pth', test_loader)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\"> Some of the original and reconstructed images are shown in Fig 5. Left column is original and right column is reconstructed. We clearly see that the CapsNet removes noise, smooths the digits and completes the digits. </div>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<table><tr><td><img src='results/original_image_test_29.png'></td>\\\n",
    "               <td><img src='results/reconstructed_image_test_29.png'> <caption><center>\\\n",
    "               <u> <font color='purple'> <font size = 4>Figure 5 </u><font color='purple'>\\\n",
    "               : Original and reconstructed digits</center></caption></td></tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Congratulations for completing this tutorial. More to come in the next two days. Hope you enjoyed. Sairam."
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
