{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Fast Image Processing Using Fully Convolutional Networks (FCN)\n",
    "<div style = \"text-align:justify\"> This exercise is based on our PyTorch implementation of the 2017 ICCV paper by [Qifeng Chen et al](https://arxiv.org/pdf/1709.00643.pdf). Unlike the previous three exercises where we always had a set of fully connected layers at the end, in this exercise we will build a fully convolutional network (FCN) to do a couple of image processing operations viz photographic style transfer and pencil drawing. You will be asked to: </div>\n",
    "\n",
    "- code the adaptive batch normalization function\n",
    "- fill a part of the FCNN class named *FastIP* and its *forward()* method\n",
    "- understand how data is preprocessed and loaded (as promised in the last exercise)\n",
    "\n",
    "So, let's march ahead!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Multi-Scale Context Aggregation [[2]](#references_cell)\n",
    "<div style = \"text-align:justify\"> Let's look back at our three exercises. In the first exercise, we predicted coordinates of 15 keypoints in the face. It is a regression problem. In the second and third exercises, we classified hand signs into digits and recognized faces respectively. These are classification problems. But most natural computer vision problems require dense prediction. For example, in semantic segmentation, we need to predict the object category associated with each pixel. How do we go about this?</div>\n",
    "<br>\n",
    "<div style = \"text-align:justify\">One way to solve dense prediction is to convert the CNN architectures for classification to dense prediction by adding upsampling layers. A typical CNN architecture integrates multi-scale information through pooling layers that successively downsamples the i/p until a global prediction is obtained. So, in order to obtain dense prediction, add upsampling layers.</div>\n",
    "<br>\n",
    "<div style = \"text-align:justify\"> Another way to approach dense prediction is to provide multiple rescaled versions of the image as input to the network and combine the predictions obtained for these multiple inputs.</div>\n",
    "<br>\n",
    "<div style = \"text-align:justify\"> Is severe downsampling necessary if our goal is dense prediction? Do we need multiple rescaled versions of the input? In [[2]](#references_cell) the authors propose a dedicated FCN architecture for dense prediction that outperform the above two approaches. This architecture aggregates multi-scale context through dilated convolutions.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Dilated Convolutions\n",
    "<div style = \"text-align:justify\"> First, let us define \"receptive field\". Suppose an image is passed through a convolutional layer with kernel_size = (3, 3). Each element in the ouput is computed using the 3x3 kernel centred at that element in the image. So, we say the receptive field of elements after 1 layer of convolution is 3x3. Now if this ouput passed through another convolutional layer with 3x3 kernel, then, with respect to first ouput the receptive field of each element in the second output is 3x3 while with respect to the input image it is 5x5. Receptive field is defined with respect to the input image. In general, receptive field of each element at the output of layer $l$ is $(l - 1)*(f - 1) + f$ where f is the kernel size. We can see that with vanilla convolution layers, the receptive field size grows linearly and hence the aggregation of multi-scale context is slow which is a severe limitation for high resolution images.</div>\n",
    "\n",
    "<div style = \"text-align:justify\"> Let us see what a dilated convolution is. The animation in Fig 1 should explain what it is.</div>\n",
    "<img src=\"images/dilation.gif\" style=\"width:300px;height:300px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 1** </u><font color='purple'>  : **Dilated convolution with dilation factor d = 2, stride s = 1 and kernel size f = 3 [(source)](http://deeplearning.net/software/theano/tutorial/conv_arithmetic.html)** </center></caption>\n",
    "<br>\n",
    "<div style = \"text-align:justify\"> As is clear from Fig 1, the kernel's elements correlate with 'd pixels' spaced elements in the image for a dilated convolution with dilation factor d. How does the receptive field size grow for dilated convolutions? Let's look at Fig 2.</div>\n",
    "<img src=\"images/dilated_recpField.png\" style=\"width:500px;height:200px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 2** </u><font color='purple'>  : **Growth of receptive field size with exponential dilation factors (f = 3, s = 1) [(source)](https://arxiv.org/pdf/1511.07122.pdf)** </center></caption>\n",
    "<br>\n",
    "<div style = \"text-align:justify\"> With dilation factors increasing from $2^{0}$ to $2^{2}$ (part (a) through (c) in Fig 2), the receptive field size grows from 3 to 15, an exponential growth. Hence exponential dilations can aggregate multi-scale context much more efficiently than vanilla convolutions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fully Convolutional Model for Image Processing\n",
    "<div style = \"text-align:justify\"> Given an image of size m x n, we want to process the image, say, transfer the style of the input image to style of a reference image or convert the input image to a pencil drawing. See Fig 3.</div>\n",
    "<img src=\"images/image_processing.png\" style=\"width:500px;height:500px;\">\n",
    "<caption><center> <u> <font color='purple'> **Figure 3** </u><font color='purple'>  : **Photographic style (left column) and Pencil drawing (right column)** </center></caption>\n",
    "<br>\n",
    "<div style = \"text-align:justify\">There are already state-of-the-art algorithms available for these operations [[3]](#references_cell) and [[4]](#references_cell). But they are computationally very expensive, especially for high resolution images. Can we approximate those existing image processing operators by FCN, achieving results on par with the original operators but in real time? Note that this is a dense prediction problem i.e both input and output are images (of same size). We also put a constraint that a single network (FCN) should work for both Photographic style and Pencil drawing i.e the parmeters and the flow of computation are shared for both the operations but for each operation the network will be trained separately and so will have different values for parameters. In fact the authors in [[1]](#references_cell) have come with a network for ten different image processing operations. Let us look at the details of this network (we will call it as *FastIP* though in paper it is called as *CAN24*) which we will be coding below. CAN stands for context aggregation networks. 24 refers to feature maps being fixed to 24 in all the intermediate layers (see below).</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Details of FastIP\n",
    "- *FastIP* has 9 layers $L^{0}$,...., $L^{8}$\n",
    "- $L^{0}$ is i/p layer with dimension m x n x 3 (color images). m and n can vary across images\n",
    "- $L^{8}$ is o/p layer with dimension m x n x 3. Input resolution is retained\n",
    "- All others layers are of dimension m x n x 24. i.e number of feature maps in the intermediate layers is fixed to   be 24\n",
    "- Each intermediate layer comprises of a dilated convolution with kernel_size = (3, 3) followed by adaptive batch     normalization and [leaky relu](http://pytorch.org/docs/master/nn.html#leakyrelu) activation. negative_slope for leaky relu is set to 0.2.\n",
    "- Dilation factors for convolutions are fixed to be 1, 2, 4, 8, 16, 32, 64, 1, 1\n",
    "- Loss function used is MSELoss\n",
    "\n",
    "<div style = \"text-align:justify\"> Groundtruth has been generated by running the matlab scripts implementing the existing state-of-the-art image processing algorithms. For eg, to generate ground truth results for photographic style transfer operation and pencil drawing operation, we ran the scripts implementing [[3]](#references_cell) and [[4]](#references_cell). Implementations were provided by the author. </div>\n",
    "\n",
    "So all things set for coding!\n",
    "\n",
    "First let's load the required packages."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.nn.functional as F\n",
    "from torch.autograd import Variable\n",
    "from torch.utils.data import Dataset, DataLoader\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import h5py\n",
    "import time\n",
    "import glob\n",
    "import cv2\n",
    "\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "%matplotlib inline"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Adaptive Batch Normalization\n",
    "<div style = \"text-align:justify\"> Before we implement the class *FastIP* we will define the class *AdaptiveBatchNorm2d* for adaptive batch normalization (ABN). Batch normalization is available in PyTorch but ABN is not available. By implementing this yourself, you will gain confidence to implement your own normalization layers, initialization functions etc. In Pytorch every layer has to be implemented as a class with a *forward* method. Before we do this let's see what ABN is. </div><br>\n",
    "\n",
    "<div style = \"text-align:justify\"> The authors found that batch normalization improved accuracy for a couple of operations but degraded the performance of other operations. Therefore they defined an ABN strategy with learnable parameters that adapts itself 'for' and 'against' batch normalization depending on the operation. The equation to implement for ABN is                               $\\psi(x) = ax + bBN(x)$ where $a$ and $b$ are learnable parameters and BN is batch normalization operator.</div>\n",
    "\n",
    "<div style = \"text-align:justify\">**You will be implementing below a class** *AdaptiveBatchNorm2d* where in the *init* method $a$ and $b$ will be created as learnable parameters and in the *forward* method the above equation will be implemented and it's output returned. For making $a$ and $b$ learnable parameters use [[torch.Tensor]](http://pytorch.org/docs/master/tensors.html) and [[nn.Parameter]](http://pytorch.org/docs/master/nn.html#parameters). Make $a$ and $b$ 4d for consistency in dim with other parameters i.e  $a$ and $b$ are of dimensions 1 x 1 x 1 x 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Replace None in the rhs by your code\n",
    "class AdaptiveBatchNorm2d(nn.Module):\n",
    "    \n",
    "    def __init__(self, num_features, eps=1e-5, momentum=0.1, affine=True):\n",
    "        super(AdaptiveBatchNorm2d, self).__init__()\n",
    "        self.bn = nn.BatchNorm2d(num_features, eps, momentum, affine)\n",
    "        self.a = None # nn.Parameter requires a tensor. Create it using torch.Tensor and supply it.\n",
    "        self.b = None\n",
    "\n",
    "    def forward(self, x):\n",
    "        # complete the forward method(). Max 2 lines of code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\"> Now let's build the *FastIP* class. Details of this class/model have already been provided above. To create dilated convolutions with dilation factor d, use [[nn.Conv2d]](http://pytorch.org/docs/master/nn.html#conv2d) and set the *dilation* argument to d. Note that at every stage from input to output, the height and width has to be retained. This means you have to choose *padding* argument accordingly. *stride* is always (1, 1). We leave it to you to compute *padding* correctly. **Complete the code below.**</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class FastIP(nn.Module):\n",
    "    \n",
    "    def __init__(self):\n",
    "        super().__init__()\n",
    "        self.conv1 = None\n",
    "        self.abn1 = None\n",
    "        self.l_relu1 = None\n",
    "        self.conv2 = None\n",
    "        self.abn2 = None\n",
    "        self.l_relu2 = None       \n",
    "        self.conv3 = None\n",
    "        self.abn3 = None\n",
    "        self.l_relu3 = None\n",
    "        self.conv4 = None\n",
    "        self.abn4 = None\n",
    "        self.l_relu4 = None\n",
    "        self.conv5 = None\n",
    "        self.abn5 = None\n",
    "        self.l_relu5 = None\n",
    "        self.conv6 = None\n",
    "        self.abn6 = None\n",
    "        self.l_relu6 = None\n",
    "        self.conv7 = None\n",
    "        self.abn7 = None\n",
    "        self.l_relu7 = None\n",
    "        self.conv8 = None\n",
    "        self.abn8 = None\n",
    "        self.l_relu8 = None\n",
    "        self.conv9 = None\n",
    "    \n",
    "    def forward(self, x):\n",
    "        # remove pass statement and complete the forward method     \n",
    "           pass    \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Let's check if you get the expected output; Run this cell\n",
    "torch.manual_seed(23)\n",
    "inputs = Variable(torch.randn(1, 3, 3, 3))\n",
    "fip = FastIP()\n",
    "outputs = fip(inputs)\n",
    "print(outputs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**Expected Output:**\n",
    "<br>\n",
    " <span style = \"color:green\">\n",
    " <br>\n",
    " Variable containing:\n",
    " <br>\n",
    "(0 ,0 ,.,.) = \n",
    " <br>\n",
    "1.00000e-02 \\*\n",
    " <br>\n",
    "  -2.8771 -2.8771 -2.8771\n",
    "   <br>\n",
    "  -2.8771 -2.8771 -2.8771\n",
    "   <br>\n",
    "  -2.8771 -2.8771 -2.8771\n",
    "   <br>\n",
    "(0 ,1 ,.,.) = \n",
    " <br>\n",
    "1.00000e-02 \\*\n",
    " <br>\n",
    "   2.5805  2.5805  2.5805\n",
    "    <br>\n",
    "   2.5805  2.5805  2.5805\n",
    "    <br>\n",
    "   2.5805  2.5805  2.5805\n",
    "    <br>\n",
    "(0 ,2 ,.,.) = \n",
    " <br>\n",
    "1.00000e-02 \\*\n",
    " <br>\n",
    "   0.0056  0.0056  0.0056\n",
    "    <br>\n",
    "   0.0056  0.0056  0.0056\n",
    "    <br>\n",
    "   0.0056  0.0056  0.0056\n",
    "    <br>\n",
    "[torch.FloatTensor of size 1x3x3x3]</span>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\"> Now, let's come to data preprocessing and loading. In PyTorch, to create a custom dataset, you have to inherit from *Dataset* class defined in *torch.utils.data module*. *Dataset* is an abstract class with two methods *len* and *get_item* . These methods have to be overrided in the inherited class. *len* should return length of the dataset i.e for eg, number of images in the dataset. *get_item* should support indexing such that dataset[i] can be used to get i$^{th}$ sample i.e it should return the i$^{th}$ image and the corresponding groundtruth (groundtruth only in case of training/validation dataset) if supplied the argument i. This makes our dataset an iterable. We will give an idea about creating a custom dataset class for the problem at hand. </div>\n",
    "<br>\n",
    "<div style = \"text-align:justify\"> Let us focus on photographic style transfer problem. Let's say our training images are in *data/MIT-Adobe_train_random/*. The images are of some resolution m x n. m and n could be varying across images for the problem at hand. Also, let's say the groundtruth for the training images (which themselves are images of same resolution as their correponding input images) are available in *data/original_results/Photographic-style/MIT-Adobe_train_random/*. </div>\n",
    "<br>\n",
    "<div style = \"text-align:justify\">In the *init* method we will extract the input and ground truth image names into two diferent lists from both the folders and keep it sorted. We have used *glob* package for this. *len* method can simply return the length of either of these lists which is same as the number of training images. *get_item(i)* will simply read the i$^{th}$ image and i$^{th}$ ground truth  whose names are already available in the lists. Reading image is done using *imread* in cv2 package. The images are then flipped across the channel dimension since *imread* in cv2 reads in BGR format and we need it in RGB format. Further, the images are normalized to [0, 1].</div>\n",
    "<br>\n",
    "<div style = \"text-align:justify\"> Now, images at hand are numpy arrays with dimension *height x width x num_of_channels* while PyTorch requires them to be tensors with dimensions *num_of_channels x height x width*. So we need to transpose the dimensions accordingly and then convert them to tensors. To convert from numpy to PyTorch tensor, use  [[torch.from_numpy]](http://pytorch.org/docs/master/search.html?q=from_numpy&check_keywords=yes&area=default). Then store the image and groundtruth tensors in a dictionary. Our custom dataset named *FastIPTrainDataset* is ready. **See the code below.**</div>\n",
    "<br>\n",
    "<div style = \"text-align:justify\"> Note that the point to keep in mind is that *len* should be implemented to return length of the dataset and *get_item(i)* should be implemented to return the i$^{th}$ item of the dataset. As long as this is taken care, you may implement the dataset differently. We just showed one way of doing here. Also many other transformations to data can be done if required through this class implementation and with other supplementary classes but we are skipping that for now. You may look at [[Data Loading Tutorial]](http://pytorch.org/tutorials/beginner/data_loading_tutorial.html) for more details.</div>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "class FastIPTrainDataset(Dataset):\n",
    "    \n",
    "    def __init__(self, task, random = True):\n",
    "        super().__init__()\n",
    "        if random:\n",
    "            \n",
    "            self.train_ip_folder = \"data/MIT-Adobe_train_random/\" \n",
    "            self.train_op_folder = \"data/original_results/\" + task + \"/MIT-Adobe_train_random/\" \n",
    "                                \n",
    "        else:\n",
    "            self.train_ip_folder = \"data/MIT-Adobe_train_480p/\"\n",
    "            self.train_op_folder = \"data/original_results/\" + task + \"/MIT-Adobe_train_480p/\"\n",
    "            \n",
    "        self.ip_image_names = sorted(glob.glob(self.train_ip_folder + \"*.png\"))\n",
    "        self.op_image_names = sorted(glob.glob(self.train_op_folder + \"*.png\"))\n",
    "        \n",
    "    def __len__(self):\n",
    "        return len(self.ip_image_names)\n",
    "    \n",
    "    def __getitem__(self, i):\n",
    "        ip_path = self.ip_image_names[i]\n",
    "        op_path = self.op_image_names[i]\n",
    "                \n",
    "        ip_image = cv2.imread(ip_path, 1)\n",
    "        op_image = cv2.imread(op_path, 1)\n",
    "        \n",
    "        ip_image = ip_image[...,::-1]\n",
    "        op_image = op_image[...,::-1]\n",
    "        \n",
    "        ip_image = np.around(np.transpose(ip_image, (2,0,1)) / 255.0, decimals=12)\n",
    "        op_image = np.around(np.transpose(op_image, (2,0,1)) / 255.0, decimals=12)\n",
    "        \n",
    "        ip_image = torch.from_numpy(ip_image).float()\n",
    "        op_image = torch.from_numpy(op_image).float()\n",
    "           \n",
    "        sample = {\"ip_image\":ip_image, \"op_image\":op_image}\n",
    "        \n",
    "        return sample"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Let's instantiate this class and iterate through some samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "fast_ip_dataset = FastIPTrainDataset('Photographic_style', random = False)\n",
    "\n",
    "fig = plt.figure()\n",
    "for i in range(len(fast_ip_dataset)): # we are using len method of our dataset\n",
    "    sample = fast_ip_dataset[i] # here we are using get_item method of our dataset\n",
    "\n",
    "    print(i, sample['ip_image'].shape, sample['op_image'].shape)\n",
    "\n",
    "    ax = plt.subplot(1, 2, i + 1)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Image #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(sample['ip_image'].numpy().transpose(1, 2, 0))\n",
    "    ax = plt.subplot(1, 2, i + 2)\n",
    "    plt.tight_layout()\n",
    "    ax.set_title('Groundtruth #{}'.format(i))\n",
    "    ax.axis('off')\n",
    "    plt.imshow(sample['op_image'].numpy().transpose(1, 2, 0))\n",
    "    break"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\"> Now that we have created the dataset, how do we load it for training? Do we manually iterate as above, group data in to mini-batches and then supply to train function? We can do so but that will not be efficient. Instead we will rely on *DataLoader* class defined in *torch.utils.data* module that provides features for batching, sampling, shuffling, multi-processing etc. You may look at [[DataLoader]](http://pytorch.org/docs/master/data.html) for details. For the problem at hand, since the images are big with minimum dimension being 480 pixels, we set batch_size to 1. Otherwise memory will be overwhelmed. See *load_dataset* function below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Run this cell\n",
    "def load_dataset(task, random = True):    \n",
    "    X = FastIPTrainDataset(task, random)\n",
    "    data_loader = DataLoader(X, batch_size = 1)\n",
    "    data_size = len(data_loader)\n",
    "    return data_loader, data_size"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now we can define train_model. If you had come till here, you should be able to follow train_model on your own."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def train_model(model, data_loader, dataset_size, criterion, optimizer):\n",
    "    \n",
    "    since = time.time()\n",
    "    train_loss_history = [] \n",
    "    num_epochs = model.num_epochs\n",
    "    \n",
    "    for epoch in range(num_epochs):\n",
    "        print('Epoch {}/{}'.format(epoch, num_epochs - 1))\n",
    "        print('-' * 10)\n",
    "        \n",
    "        model.train(True)\n",
    "        \n",
    "        running_loss = 0.0\n",
    "        \n",
    "        for data in data_loader:\n",
    "            ip_imgs = data[\"ip_image\"]\n",
    "            orig_op_imgs = data[\"op_image\"]\n",
    "            \n",
    "            if ip_imgs.size()[0] * ip_imgs.size()[1] > 2200000:\n",
    "                continue                             \n",
    "            ip_imgs, orig_op_imgs = Variable(ip_imgs), Variable(orig_op_imgs)\n",
    "            \n",
    "            # Dont' uncomment the 4 commented lines below. We will be working with CPU.\n",
    "            \n",
    "            #if torch.cuda.is_available():\n",
    "             #   ip_imgs, orig_op_imgs = Variable(ip_imgs.cuda()), Variable(orig_op_imgs.cuda())\n",
    "            #else:\n",
    "             #   ip_imgs, orig_op_imgs = Variable(ip_imgs), Variable(orig_op_imgs)\n",
    "            \n",
    "            optimizer.zero_grad()\n",
    "            op_imgs = model.forward(ip_imgs)\n",
    "            \n",
    "            loss = criterion(op_imgs, orig_op_imgs)\n",
    "        \n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "            \n",
    "            running_loss += loss.data[0]\n",
    "            \n",
    "        epoch_loss = running_loss / dataset_size           \n",
    "        train_loss_history.append(epoch_loss)        \n",
    "        print('Train Loss: {:.8f}'.format(epoch_loss))\n",
    "        \n",
    "    time_elapsed = time.time() - since\n",
    "    print('Training complete in {:.0f}m {:.0f}s'.format(\n",
    "            time_elapsed // 60, time_elapsed % 60))    \n",
    "    return"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# load training data\n",
    "data_loader, data_size = load_dataset('Photographic_style', random = False)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\"> We have trained two separate models one for photographic style transfer and the other for pencil drawing on the MIT-Adobe dataset [[5]](#reference_cell). Each of them took around 33 to 35 hours of training for 180 epochs (close to 500K iterations) on Titan-X GPU. It is roughly around 9-10 minutes per epoch. Since you are working on CPU now, it will take a very long time even to train one epoch. So we are not asking you to train. We generated results for both photographic style transfer and pencil drawing using our pre-trained models. A couple of results are shown in Figures 4 and 5. For more results, Click on File-->Open-->results and see some of the results generated for both the tasks.</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style = \"text-align:justify\"> By the way, why was the title \"Fast Image Processing using FCN\"? The reason is that doing photographic style transfer and pencil drawing using traditional hand-crafted algorithms even with parallelization (MATLAB parallel toolbox) consumes around 6000 and 5000 milliseconds respectively [[1]](#references_cell)  for images at 1080p resolution from MIT-Adobe-Test set [[5]](#references_cell) while FCN consumes constant 190 milliseconds for both the operations. Runtime was measured on a workstation with an Intel i7-5960X 3.0GHz CPU and an Nvidia Titan-X GPU [[1]](#references_cell). It's super fast \"Chennai Express\"!!!\n",
    "\n",
    "Hearty Congratulations!! Drained out or pumped up??? Any way, we are planning to cover the latest in CNN - just 30 days old, after dinner at 7.45 p.m. - Capsule Networks!!! But the session is optional. Let us see how many turn up. \n",
    "\n",
    "Wish you all the best and lots of love. Thank You for your cooperation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='results/Photographic_style/001166.png'></td>               <td><img src='results/Photographic_style/001166.jpg'> <caption><center>               <u> <font color='purple'> <font size = 4>Figure 4 </u><font color='purple'>               : Photographic style transfer</center></caption></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<table><tr><td><img src='results/Pencil_drawing/000004.png'></td>               <td><img src='results/Pencil_drawing/000004.jpg'> <caption><center>               <u> <font color='purple'> <font size = 4>Figure 5 </u><font color='purple'>               : Pencil Drawing</center></caption></td></tr></table>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from IPython.display import HTML, display\n",
    "display(HTML(\"<table><tr><td><img src='results/Photographic_style/001166.png'></td>\\\n",
    "               <td><img src='results/Photographic_style/001166.jpg'> <caption><center>\\\n",
    "               <u> <font color='purple'> <font size = 4>Figure 4 </u><font color='purple'>\\\n",
    "               : Photographic style transfer</center></caption></td></tr></table>\"))\n",
    "display(HTML(\"<table><tr><td><img src='results/Pencil_drawing/000004.png'></td>\\\n",
    "               <td><img src='results/Pencil_drawing/000004.jpg'> <caption><center>\\\n",
    "               <u> <font color='purple'> <font size = 4>Figure 5 </u><font color='purple'>\\\n",
    "               : Pencil Drawing</center></caption></td></tr></table>\"))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "<a id='references_cell'></a>\n",
    "### References\n",
    "1. [Qifeng Chen, Jia Xu and Vladlen Koltun - Fast Image Processing with Fully-Convolutional Networks (2017)](https://arxiv.org/pdf/1709.00643.pdf)\n",
    "\n",
    "2. [Fisher Yu and Vladelen Koltun - Multi-Scale Context Aggregations by Dilated Convolutions (2016)](https://arxiv.org/pdf/1511.07122.pdf)\n",
    "\n",
    "3. [M. Aubry, S. Paris, S. W. Hasinoff, J. Kautz, and F. Durand - Fast local Laplacian filters: Theory and              applications (2014)](people.csail.mit.edu/sparis/publi/2014/tog/Aubry_14-Fast_Local_Laplacian_Filters.pdf\n",
    ")\n",
    "\n",
    "4. [C. Lu, L. Xu, and J. Jia - Combining sketch and tone for pencil drawing production (2012)](www.cse.cuhk.edu.hk/leojia/projects/pencilsketch/npar12_pencil.pdf)\n",
    "\n",
    "5. [V. Bychkovsky, S. Paris, E. Chan, and F. Durand - Learning photographic global tonal adjustment with a database of input / output image pairs (2011).](http://citeseerx.ist.psu.edu/viewdoc/download?doi=10.1.1.208.67&rep=rep1&type=pdf)"
   ]
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Environment (conda_tensorflow)",
   "language": "python",
   "name": "conda_tensorflow"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
